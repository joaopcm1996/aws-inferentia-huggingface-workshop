{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Summit San Francisco 2022\n",
    "## Using AWS Inferentia to optimize HuggingFace model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the AWS Summit San Francisco 2022 Inferentia Workshop! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setting up the environment](#setenv)\n",
    "3. [Get model from HuggingFace Model Hub](#getmodel)\n",
    "    1. [Get the Tokenizer](#gettoken)\n",
    "    2. [Create a trace for Sagemaker Neo compiler](#trace)\n",
    "4. [Deploy default model to a CPU-based endpoint](#deploycpu)\n",
    "    1. [Perform a test CPU based inference](#testcpu)\n",
    "5. [Compile and deploy the model on an Inferentia instance](#compiledeploy)\n",
    "    1. [Review changes to the inference code](#reviewchanges)\n",
    "    2. [Create and compile Pytorch model for the inf1 instance](#pytorchmodel)\n",
    "    3. [Deploy compiled model into the inf1 instance](#deployinf1)\n",
    "    4. [Perform a test inf1 based inference](#testinf1)\n",
    "6. [Benchmark and comparison](#benchmark)\n",
    "    1. [Benchmark CPU based endpoint](#benchcpu)\n",
    "    2. [Benchmark Inferentia based endpoint](#benchinf1)\n",
    "    3. [Comparison and conclusions](#conclusions)\n",
    "7. [Cleanup](#cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this workshop, we will deploy 2 HuggingFace NLP models for the task of paraphrase detection on SageMaker endpoints. Paraphrase detection is an NLP classification problem. Given a pair of sentences, the system determines. the semantic similarity between the two sentences. If the two sentences convey the same meaning it is. labelled as paraphrase, otherwise non-paraphrase. We will use `distilbert-base-uncased` [DistilBERT](https://huggingface.co/distilbert-base-uncased) transfromer model from HuggingFace Model Hub. \n",
    "These 2 models will be deployed in different instances: a CPU-based instance, with no changes to the model; and the other will be compiled and deployed to an Inf1 instance on SageMaker. Finally, we will perform a latency and throughput performance comparison of both endpoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/) is Amazon's first custom silicon designed to accelerate deep learning workloads and is part of a long-term strategy to deliver on this vision. AWS Inferentia is designed to provide high performance inference in the cloud, to drive down the total cost of inference, and to make it easy for developers to integrate machine learning into their business applications. AWS Inferentia chips deliver up 2.3x higher throughput and up to 70% lower cost per inference than comparable current generation GPU-based Amazon EC2 instances, as we will confirm in the example notebook.\n",
    "\n",
    "[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/) is a software development kit (SDK) for running machine learning inference using AWS Inferentia chips. It consists of a compiler, run-time, and profiling tools that enable developers to run high-performance and low latency inference using AWS Inferentia-based Amazon EC2 Inf1 instances. Using Neuron, you can bring your models that have been trained on any popular framework (PyTorch, TensorFlow, MXNet), and run them optimally on Inferentia. There is excellent support for Vision and NLP models especially, and on top of that we have released great features to help you make the most efficient use of the hardware, such as [dynamic batching](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/appnotes/perf/torch-neuron-dataparallel-app-note.html#dynamic-batching-description) or [Data Parallel](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/api-torch-neuron-dataparallel-api.html) inferencing.\n",
    "\n",
    "[SageMaker Neo](https://aws.amazon.com/sagemaker/neo/) saves you the effort of DIY model compilation, extending familiar SageMaker SDK API's to enable easy compilation for a [wide range](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#API_OutputConfig_Contents) of platforms. This includes CPU and GPU-based instances, but also Inf1 instances; in this case, SageMaker Neo uses the Neuron SDK to compile your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started  <a name=\"setenv\"></a>\n",
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install required Python packages. Also, we will create a default Amazon Sagemaker session, get the Amazon Sagemaker role and default Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers\n",
    "!pip install -U sagemaker\n",
    "!pip install -U torch\n",
    "\n",
    "import IPython\n",
    "import sys\n",
    "\n",
    "#!{sys.executable} -m pip install ipywidgets\n",
    "#IPython.Application.instance().kernel.do_shutdown(True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP! Restart kernel before continuing so new packages can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import sagemaker\n",
    "import torch\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sess_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model from HuggingFace Model Hub <a name=\"getmodel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we chose one of the most downloaded models from the HuggingFace Model Hub: `distilbert-base-uncased`. [DistilBERT](https://huggingface.co/distilbert-base-uncased). It is a transformer model, smaller and faster than BERT, which was pretrained on the same data as BERT, which is BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Tokenizer <a name=\"gettoken\"></a>\n",
    "We need to get the tokenizer in order to create a sample input to trace the model. We get it from HuggingFace through the `transformers` library. It is important to set the `return_dict` parameter to `False` when instantiating the model. In `transformers` v4.x, this parameter is `True` by default and it enables the return of dict-like python objects containing the model outputs, instead of the standard tuples. Neuron compilation does not support dictionary-based model ouputs, and compilation would fail if we didn't explictly set it to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", return_dict=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a trace for Sagemaker Neo compiler <a name=\"trace\"></a>\n",
    "PyTorch models must be saved as a definition file (.pt or .pth) with input datatype of float32.\n",
    "\n",
    "To save your model, use torch.jit.trace followed by torch.save. This will save an object to a disk file and by default uses python pickle (pickle_module=pickle) to save the objects and some metadata. Next, convert the saved model to a compressed tar file.\n",
    "\n",
    "We will create a sample input to `jit.trace` of the model with PyTorch; this is a required step to have SageMaker Neo compile the model artifact, which will take a `tar.gz` file containing the traced model. We will upload the `tar.gz` file to an Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Create directory for model artifacts\n",
    "Path(\"traced_model/\").mkdir(exist_ok=True)\n",
    "\n",
    "# Prepare sample input for jit model tracing\n",
    "seq_0 = \"Welcome to AWS Summit San Francisco 2022! Thank you for attending the workshop on using Huggingface transformers on Inferentia instances.\"\n",
    "seq_1 = seq_0\n",
    "max_length = 512\n",
    "\n",
    "tokenized_sequence_pair = tokenizer.encode_plus(\n",
    "    seq_0, seq_1, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "example = tokenized_sequence_pair[\"input_ids\"], tokenized_sequence_pair[\"attention_mask\"]\n",
    "\n",
    "traced_model = torch.jit.trace(model.eval(), example)\n",
    "traced_model.save(\"traced_model/model.pth\") # The `.pth` extension is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./model.pth\n",
      "./traced_model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf traced_model.tar.gz -C traced_model . && mv traced_model.tar.gz traced_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload the traced model `tar.gz` file to Amazon S3, where the compilation job will download it from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model_url = sagemaker_session.upload_data(\n",
    "    path=\"traced_model/traced_model.tar.gz\",\n",
    "    key_prefix=\"neuron-experiments/bert-seq-classification/traced-model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy default model to a CPU-based endpoint <a name=\"deploycpu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be passing the `traced_model_url` as the `model_data` parameter to the `PyTorchModel` API. And will be pulling the model from the HuggingFace Model Hub in the inference script; this won't affect the benchmark since `model_fn` gets executed before any request reaches the endpoint. \n",
    "We are using `PyTorchModel` here instead of the [HuggingFace specific](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model) (and optimized) [`HuggingFaceModel`](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model) because the latter is not integrated with SageMaker Neo at the time. \n",
    "\n",
    "Notice that we are passing `inference_normal.py` as the entry point script; also, the packages defined in the requirements file within the `source_dir` will automatically be installed in the endpoint instance. In this case we will use the `transformers` library that is compatible Inferentia instances (v. 4.15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from datetime import datetime\n",
    "\n",
    "prefix = \"neuron-experiments/bert-seq-classification\"\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "normal_sm_model = PyTorchModel(\n",
    "    model_data=traced_model_url,\n",
    "    predictor_cls=Predictor,\n",
    "    framework_version=\"1.8.1\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    entry_point=\"inference_normal.py\",\n",
    "    source_dir=\"code\",\n",
    "    py_version=\"py3\",\n",
    "    name=f\"distilbert-pt181-{date_string}\",\n",
    "    env={\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"10\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!CPU times: user 29 s, sys: 5.61 s, total: 34.6 s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "hardware = \"c4\"\n",
    "\n",
    "normal_predictor = normal_sm_model.deploy(\n",
    "    instance_type=\"ml.c4.2xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=f\"distilbert-{hardware}-{date_string}\",\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a CPU based test inference <a name=\"testcpu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a quick test to see if the endpoint is responding as expected. We will send sample sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT predicts that \"Welcome to AWS Summit San Francisco 2022! Thank you for attending the workshop on using Huggingface transformers on Inferentia instances.\" and \"Welcome to AWS Summit San Francisco 2022! Thank you for attending the workshop on using Huggingface transformers on Inferentia instances.\" are paraphrase'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = seq_0, seq_1\n",
    "normal_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and deploy the model on an Inferentia instance <a name=\"compiledeploy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will cover the compilation and deployment of the model into the inf1 instance. We will also review the changes in the inference code that are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review inference code <a name=\"reviewchanges\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the changes needed in the inference code to perform inferences in inf1 instances, let's review the code for the uncompiled model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
      "\n",
      "JSON_CONTENT_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "device = torch.device(\u001b[33m'\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "\n",
      "    tokenizer_init = AutoTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).eval().to(device)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m (model, tokenizer_init)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type=JSON_CONTENT_TYPE):\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == JSON_CONTENT_TYPE:\n",
      "        input_data = json.loads(serialized_input_data)\n",
      "        \u001b[34mreturn\u001b[39;49;00m input_data\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\n",
      "        \u001b[34mreturn\u001b[39;49;00m\n",
      "    \n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, models):\n",
      "\n",
      "    model_bert, tokenizer = models\n",
      "    sequence_0 = input_data[\u001b[34m0\u001b[39;49;00m] \n",
      "    sequence_1 = input_data[\u001b[34m1\u001b[39;49;00m]\n",
      "    \n",
      "    max_length = \u001b[34m512\u001b[39;49;00m\n",
      "    tokenized_sequence_pair = tokenizer.encode_plus(sequence_0,\n",
      "                                                    sequence_1,\n",
      "                                                    max_length=max_length,\n",
      "                                                    padding=\u001b[33m'\u001b[39;49;00m\u001b[33mmax_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                                                    truncation=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                                                    return_tensors=\u001b[33m'\u001b[39;49;00m\u001b[33mpt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).to(device)\n",
      "    \n",
      "    \u001b[37m# Convert example inputs to a format that is compatible with TorchScript tracing\u001b[39;49;00m\n",
      "    example_inputs = tokenized_sequence_pair[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], tokenized_sequence_pair[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        paraphrase_classification_logits = model_bert(*example_inputs)\n",
      "    \n",
      "    classes = [\u001b[33m'\u001b[39;49;00m\u001b[33mnot paraphrase\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mparaphrase\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    paraphrase_prediction = paraphrase_classification_logits[\u001b[34m0\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m].argmax().item()\n",
      "    out_str = \u001b[33m'\u001b[39;49;00m\u001b[33mBERT predicts that \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m and \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m are \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(sequence_0, sequence_1, classes[paraphrase_prediction])\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m out_str\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept=JSON_CONTENT_TYPE):\n",
      "    \u001b[34mif\u001b[39;49;00m accept == JSON_CONTENT_TYPE:\n",
      "        \u001b[34mreturn\u001b[39;49;00m json.dumps(prediction_output), accept\n",
      "    \n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + accept)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/inference_normal.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_fn` receives the model directory, is responsible for loading and returning the model -, `input_fn` and `output_fn` functions are in charge of pre-processing/checking content types of input and output to the endpoint - and a `predict_fn`, receives the outputs of `model_fn` and `input_fn` (meaning, the loaded model and the deserialized/pre-processed input data) and defines how the model will run inference.\n",
    "\n",
    "In this case, notice that we will load the model directly from the HuggingFace Model Hub for simplicity. `model_fn` will return a tuple containing both the model and its corresponding tokenizer. Both the model and the input data will be sent `.to(device)`, which can be a CPU or GPU, as we can see in line 7 of the file.\n",
    "Now, lets see what changes in the inference code when we want to do inference with a model that has been compiled for Inferentia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s model_fn code/inference_inf1.py\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    model_dir = '/opt/ml/model/'\n",
    "    dir_contents = os.listdir(model_dir)\n",
    "    model_path = next(filter(lambda item: 'model' in item, dir_contents), None)\n",
    "    \n",
    "    tokenizer_init = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    model = torch.jit.load(os.path.join(model_dir, model_path))\n",
    "\n",
    "    \n",
    "    return (model, tokenizer_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s model_fn code/inference_inf1.py\n",
    "def model_fn(model_dir):\n",
    "\n",
    "    dir_contents = os.listdir(model_dir)\n",
    "    model_path = next(filter(lambda item: \"model\" in item, dir_contents), None)\n",
    "\n",
    "    tokenizer_init = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = torch.jit.load(os.path.join(model_dir, model_path))\n",
    "\n",
    "    return (model, tokenizer_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, within the `model_fn` we first grab the model artifact located in `model_dir` (the compilation step will name the artifact `model_neuron.pt`, but we just get the first file containing `model` in its name for script flexibility). Then, **we load the Neuron compiled model with `torch.jit.load`**. \n",
    "\n",
    "Other than this change to `model_fn`, we only need to add an extra import `import torch_neuron` to the beginning of the script, and remove of all `.to(device)` calls, since the Neuron runtime will take care of loading the model to the NeuronCores on the Inferentia instance. All other functions are unchanged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and compile Pytorch model for the inf1 instance <a name=\"pytorchmodel\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a new `PyTorchModel` that will use the `inference_inf1.py` file described above as its entry point script. PyTorch version 1.5.1 is the latest that supports Neo compilation to Inferentia, as you can see from the warning in the compilation cell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"neuron-experiments/bert-seq-classification\"\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "compiled_sm_model = PyTorchModel(\n",
    "    model_data=traced_model_url,\n",
    "    predictor_cls=Predictor,\n",
    "    framework_version=\"1.5.1\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    entry_point=\"inference_inf1.py\",\n",
    "    source_dir=\"code\",\n",
    "    py_version=\"py3\",\n",
    "    name=f\"distilbert-pt181-{date_string}\",\n",
    "    env={\"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"10\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to compile the model! Two additional notes:\n",
    "* HuggingFace models should be compiled to `dtype` `int64`\n",
    "* the format for `compiler_options` differs from the standard Python `dict` that you can use when compiling for \"normal\" instance types; for inferentia, you must provide a JSON string with CLI arguments, which correspond to the ones supported by the [Neuron Compiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-cc/command-line-reference.html) (read more about `compiler_options` [here](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#API_OutputConfig_Contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "???????????????????????????????.............................................................................................!CPU times: user 468 ms, sys: 61 ms, total: 529 ms\n",
      "Wall time: 10min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "hardware = \"inf1\"\n",
    "compilation_job_name = f\"distilbert-{hardware}-\" + date_string\n",
    "\n",
    "compiled_inf1_model = compiled_sm_model.compile(\n",
    "    target_instance_family=f\"ml_{hardware}\",\n",
    "    input_shape={\"input_ids\": [1, 512], \"attention_mask\": [1, 512]},\n",
    "    job_name=compilation_job_name,\n",
    "    role=role,\n",
    "    framework=\"pytorch\",\n",
    "    framework_version=\"1.5.1\",\n",
    "    output_path=f\"s3://{sess_bucket}/{prefix}/neo-compilations/{hardware}-model\",\n",
    "    compiler_options=json.dumps(\"--dtype int64\"),\n",
    "    compile_max_run=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy compiled model into the inf1 instance <a name=\"deployinf1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successful compilation, we deploy the new model to an inf1.xlarge instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 10.3 s, sys: 1.86 s, total: 12.1 s\n",
      "Wall time: 6min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "date_string = datetime.now().strftime(\"%Y%m-%d%H-%M%S\")\n",
    "\n",
    "compiled_inf1_predictor = compiled_inf1_model.deploy(\n",
    "    instance_type=\"ml.inf1.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=f\"test-neo-{hardware}-{date_string}\",\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a test inference <a name=\"testinf1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test if everything is running OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BERT predicts that \"Welcome to AWS Summit San Francisco 2022! Thank you for attending the workshop on using Huggingface transformers on Inferentia instances.\" and \"Welcome to AWS Summit San Francisco 2022! Thank you for attending the workshop on using Huggingface transformers on Inferentia instances.\" are paraphrase'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict with model endpoint\n",
    "payload = seq_0, seq_1\n",
    "compiled_inf1_predictor.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark and comparison <a name=\"benchmark\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both endpoints online, we will now perform a benchmark using Python's `threading` module. In each benchmark, we start 5 threads that will each make 300 requests to the model endpoint. We measure the inference latency for each request, and we also measure the total time to finish the task, so that we can get an estimate of the request throughput/second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark CPU based endpoint <a name=\"benchcpu\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 140625342985984 started\n",
      "Thread 140625837516544 started\n",
      "Thread 140625862694656 started\n",
      "Thread 140625829123840 started\n",
      "Thread 140625879480064 started\n",
      "CPU times: user 3.51 s, sys: 141 ms, total: 3.65 s\n",
      "Wall time: 5min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run the benchmark \n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "num_preds = 300\n",
    "num_threads = 5\n",
    "\n",
    "times = []\n",
    "\n",
    "\n",
    "def predict():\n",
    "    thread_id = threading.get_ident()\n",
    "    print(f\"Thread {thread_id} started\")\n",
    "\n",
    "    for i in range(num_preds):\n",
    "        tick = time.time()\n",
    "        response = normal_predictor.predict(payload)\n",
    "        tock = time.time()\n",
    "        times.append((thread_id, tock - tick))\n",
    "\n",
    "\n",
    "threads = []\n",
    "[threads.append(threading.Thread(target=predict, daemon=False)) for i in range(num_threads)]\n",
    "[t.start() for t in threads]\n",
    "\n",
    "# Wait for threads, get an estimate of total time\n",
    "start = time.time()\n",
    "[t.join() for t in threads]\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Default HuggingFace model on CPU benchmark ====\n",
      "\n",
      "95 % of requests take less than 1172.8814244270325 ms\n",
      "Rough request throughput/second is 4.54340184162512\n"
     ]
    }
   ],
   "source": [
    "# Display results \n",
    "from matplotlib.pyplot import hist, title, show, xlim\n",
    "import numpy as np\n",
    "\n",
    "TPS = (num_preds * num_threads) / end\n",
    "\n",
    "t = [duration for thread__id, duration in times]\n",
    "latency_percentiles = np.percentile(t, q=[50, 90, 95, 99])\n",
    "\n",
    "hist(t, bins=100)\n",
    "title(\"Request latency histogram on CPU\")\n",
    "xlim(0.8, 1.6)\n",
    "show()\n",
    "\n",
    "print(\"==== Default HuggingFace model on CPU benchmark ====\\n\")\n",
    "print(f\"95 % of requests take less than {latency_percentiles[2]*1000} ms\")\n",
    "print(f\"Rough request throughput/second is {TPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that request latency is in the 1-1.2 second range, and throughput is ~4.5 TPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Inferentia based endpoint <a name=\"benchinf1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 140625879480064 started\n",
      "Thread 140625837516544 started\n",
      "Thread 140625829123840 started\n",
      "Thread 140625862694656 started\n",
      "Thread 140625851594496 started\n",
      "CPU times: user 2.97 s, sys: 152 ms, total: 3.12 s\n",
      "Wall time: 9.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run benchmark \n",
    "\n",
    "import threading\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "num_preds = 300\n",
    "num_threads = 5\n",
    "\n",
    "times = []\n",
    "\n",
    "\n",
    "def predict():\n",
    "    thread_id = threading.get_ident()\n",
    "    print(f\"Thread {thread_id} started\")\n",
    "\n",
    "    for i in range(num_preds):\n",
    "        tick = time.time()\n",
    "        response = compiled_inf1_predictor.predict(payload)\n",
    "        tock = time.time()\n",
    "        times.append((thread_id, tock - tick))\n",
    "\n",
    "\n",
    "threads = []\n",
    "[threads.append(threading.Thread(target=predict, daemon=False)) for i in range(num_threads)]\n",
    "[t.start() for t in threads]\n",
    "\n",
    "# Make a rough estimate of total time, wait for threads\n",
    "start = time.time()\n",
    "[t.join() for t in threads]\n",
    "end = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== HuggingFace model compiled for Inferentia benchmark ====\n",
      "\n",
      "95 % of requests take less than 36.40700578689575 ms\n",
      "Rough request throughput/second is 158.36348549205135\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "\n",
    "from matplotlib.pyplot import hist, title, show, savefig, xlim\n",
    "import numpy as np\n",
    "\n",
    "TPS = (num_preds * num_threads) / end\n",
    "\n",
    "t = [duration for thread__id, duration in times]\n",
    "latency_percentiles = np.percentile(t, q=[50, 90, 95, 99])\n",
    "\n",
    "hist(t, bins=100)\n",
    "title(\"Request latency histogram for Inferentia\")\n",
    "xlim(0, 0.1)\n",
    "show()\n",
    "\n",
    "print(\"==== HuggingFace model compiled for Inferentia benchmark ====\\n\")\n",
    "print(f\"95 % of requests take less than {latency_percentiles[2]*1000} ms\")\n",
    "print(f\"Rough request throughput/second is {TPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that request latency is in the 0.02-0.05 millisecond range, and throughput is ~157 TPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <a name=\"conclusions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using inf1 instances latency dropped to 25-30 millisecond range while throughput has increased to 220 TPS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increase in performance obtained from using inf1 instances, paired with the cost reduction and the use of known SageMaker SDK APIs, enables new benefits with little development effort and a gentle learning curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up <a name=\"cleanup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_predictor.delete_model()\n",
    "normal_predictor.delete_endpoint()\n",
    "compiled_inf1_predictor.delete_model()\n",
    "compiled_inf1_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
